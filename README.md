# Neural Architecture Search for Deep Sensor Fusion


Fusion of data from multiple sensors plays a pivotal role for improving the performance of object detection in autonomous vehicles. This becomes particularly relevant in adverse weather conditions because multimodal deep sensor fusion can exploit redundancies in multi-sensor data and perform better due to the fact that sensors can fail asymmetrically in such conditions. To this end, designing effective and efficient neural networks for multimodal feature fusion is extremely important for obtaining good performance. This handcrafted design by experts is typically task-specific and it requires a colossal number of computational resources, time, and memory to arrive at an optimal architecture. Automated Neural Architecture Search (NAS) is a promising area to explore to overcome these issues. However, optimizing a network for a job is a tedious task that requires lengthy search time, high processor needs, and a thorough examination of enormous possibilities. The need of the hour is to develop a strategy that saves time while maintaining an excellent level of accuracy in deep multimodal learning. In this project, we want to design, explore, and experiment with effective NAS methods for multimodal sensor fusion, in the task of object detection, which are also memory, time, and compute efficient.
