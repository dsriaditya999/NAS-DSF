{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models import load_checkpoint\n",
    "import effdet\n",
    "from effdet import EfficientDet\n",
    "from effdet.efficientdet import get_feature_info\n",
    "from effdet import DetBenchTrain\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import List, Callable, Optional, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm import create_model\n",
    "from timm.models.layers import create_conv2d, create_pool2d, get_act_layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models import load_checkpoint\n",
    "import effdet\n",
    "from effdet import EfficientDet\n",
    "from effdet.efficientdet import get_feature_info\n",
    "from effdet import DetBenchTrain\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import itertools\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# from .config import get_fpn_config\n",
    "\n",
    "_DEBUG = False\n",
    "_USE_SCALE = False\n",
    "_ACT_LAYER = get_act_layer('silu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SequentialList(nn.Sequential):\n",
    "    \"\"\" This module exists to work around torchscript typing issues list -> list\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(SequentialList, self).__init__(*args)\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, padding='', bias=False,\n",
    "                 norm_layer=nn.BatchNorm2d, act_layer=_ACT_LAYER):\n",
    "        super(ConvBnAct2d, self).__init__()\n",
    "        self.conv = create_conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias)\n",
    "        self.bn = None if norm_layer is None else norm_layer(out_channels)\n",
    "        self.act = None if act_layer is None else act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\" Separable Conv\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding='', bias=False,\n",
    "                 channel_multiplier=1.0, pw_kernel_size=1, norm_layer=nn.BatchNorm2d, act_layer=_ACT_LAYER):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.conv_dw = create_conv2d(\n",
    "            in_channels, int(in_channels * channel_multiplier), kernel_size,\n",
    "            stride=stride, dilation=dilation, padding=padding, depthwise=True)\n",
    "\n",
    "        self.conv_pw = create_conv2d(\n",
    "            int(in_channels * channel_multiplier), out_channels, pw_kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "        self.bn = None if norm_layer is None else norm_layer(out_channels)\n",
    "        self.act = None if act_layer is None else act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.conv_pw(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Interpolate2d(nn.Module):\n",
    "    r\"\"\"Resamples a 2d Image\n",
    "    The input data is assumed to be of the form\n",
    "    `minibatch x channels x [optional depth] x [optional height] x width`.\n",
    "    Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.\n",
    "    The algorithms available for upsampling are nearest neighbor and linear,\n",
    "    bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,\n",
    "    respectively.\n",
    "    One can either give a :attr:`scale_factor` or the target output :attr:`size` to\n",
    "    calculate the output size. (You cannot give both, as it is ambiguous)\n",
    "    Args:\n",
    "        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional):\n",
    "            output spatial sizes\n",
    "        scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional):\n",
    "            multiplier for spatial size. Has to match input size if it is a tuple.\n",
    "        mode (str, optional): the upsampling algorithm: one of ``'nearest'``,\n",
    "            ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``.\n",
    "            Default: ``'nearest'``\n",
    "        align_corners (bool, optional): if ``True``, the corner pixels of the input\n",
    "            and output tensors are aligned, and thus preserving the values at\n",
    "            those pixels. This only has effect when :attr:`mode` is\n",
    "            ``'linear'``, ``'bilinear'``, or ``'trilinear'``. Default: ``False``\n",
    "    \"\"\"\n",
    "    __constants__ = ['size', 'scale_factor', 'mode', 'align_corners', 'name']\n",
    "    name: str\n",
    "    size: Optional[Union[int, Tuple[int, int]]]\n",
    "    scale_factor: Optional[Union[float, Tuple[float, float]]]\n",
    "    mode: str\n",
    "    align_corners: Optional[bool]\n",
    "\n",
    "    def __init__(self,\n",
    "                 size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "                 scale_factor: Optional[Union[float, Tuple[float, float]]] = None,\n",
    "                 mode: str = 'nearest',\n",
    "                 align_corners: bool = False) -> None:\n",
    "        super(Interpolate2d, self).__init__()\n",
    "        self.name = type(self).__name__\n",
    "        self.size = size\n",
    "        if isinstance(scale_factor, tuple):\n",
    "            self.scale_factor = tuple(float(factor) for factor in scale_factor)\n",
    "        else:\n",
    "            self.scale_factor = float(scale_factor) if scale_factor else None\n",
    "        self.mode = mode\n",
    "        self.align_corners = None if mode == 'nearest' else align_corners\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.interpolate(\n",
    "            input, self.size, self.scale_factor, self.mode, self.align_corners, recompute_scale_factor=False)\n",
    "\n",
    "\n",
    "class ResampleFeatureMap(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, input_size, output_size, pad_type='',\n",
    "            downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, apply_bn=False, redundant_bias=False):\n",
    "        super(ResampleFeatureMap, self).__init__()\n",
    "        downsample = downsample or 'max'\n",
    "        upsample = upsample or 'nearest'\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.add_module('conv', ConvBnAct2d(\n",
    "                in_channels, out_channels, kernel_size=1, padding=pad_type,\n",
    "                norm_layer=norm_layer if apply_bn else None,\n",
    "                bias=not apply_bn or redundant_bias, act_layer=None))\n",
    "\n",
    "        if input_size[0] > output_size[0] and input_size[1] > output_size[1]:\n",
    "            if downsample in ('max', 'avg'):\n",
    "                stride_size_h = int((input_size[0] - 1) // output_size[0] + 1)\n",
    "                stride_size_w = int((input_size[1] - 1) // output_size[1] + 1)\n",
    "                if stride_size_h == stride_size_w:\n",
    "                    kernel_size = stride_size_h + 1\n",
    "                    stride = stride_size_h\n",
    "                else:\n",
    "                    # FIXME need to support tuple kernel / stride input to padding fns\n",
    "                    kernel_size = (stride_size_h + 1, stride_size_w + 1)\n",
    "                    stride = (stride_size_h, stride_size_w)\n",
    "                down_inst = create_pool2d(downsample, kernel_size=kernel_size, stride=stride, padding=pad_type)\n",
    "            else:\n",
    "                if _USE_SCALE:  # FIXME not sure if scale vs size is better, leaving both in to test for now\n",
    "                    scale = (output_size[0] / input_size[0], output_size[1] / input_size[1])\n",
    "                    down_inst = Interpolate2d(scale_factor=scale, mode=downsample)\n",
    "                else:\n",
    "                    down_inst = Interpolate2d(size=output_size, mode=downsample)\n",
    "            self.add_module('downsample', down_inst)\n",
    "        else:\n",
    "            if input_size[0] < output_size[0] or input_size[1] < output_size[1]:\n",
    "                if _USE_SCALE:\n",
    "                    scale = (output_size[0] / input_size[0], output_size[1] / input_size[1])\n",
    "                    self.add_module('upsample', Interpolate2d(scale_factor=scale, mode=upsample))\n",
    "                else:\n",
    "                    self.add_module('upsample', Interpolate2d(size=output_size, mode=upsample))\n",
    "\n",
    "\n",
    "class FpnCombine(nn.Module):\n",
    "    def __init__(self, feature_info, fpn_channels, inputs_offsets, output_size, pad_type='',\n",
    "                 downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, apply_resample_bn=False,\n",
    "                 redundant_bias=False, weight_method='attn'):\n",
    "        super(FpnCombine, self).__init__()\n",
    "        self.inputs_offsets = inputs_offsets\n",
    "        self.weight_method = weight_method\n",
    "\n",
    "        self.resample = nn.ModuleDict()\n",
    "        for idx, offset in enumerate(inputs_offsets):\n",
    "            self.resample[str(offset)] = ResampleFeatureMap(\n",
    "                feature_info[offset]['num_chs'], fpn_channels,\n",
    "                input_size=feature_info[offset]['size'], output_size=output_size, pad_type=pad_type,\n",
    "                downsample=downsample, upsample=upsample, norm_layer=norm_layer, apply_bn=apply_resample_bn,\n",
    "                redundant_bias=redundant_bias)\n",
    "\n",
    "        if weight_method == 'attn' or weight_method == 'fastattn':\n",
    "            self.edge_weights = nn.Parameter(torch.ones(len(inputs_offsets)), requires_grad=True)  # WSM\n",
    "        else:\n",
    "            self.edge_weights = None\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        dtype = x[0].dtype\n",
    "        nodes = []\n",
    "        for offset, resample in zip(self.inputs_offsets, self.resample.values()):\n",
    "            input_node = x[offset]\n",
    "            input_node = resample(input_node)\n",
    "            nodes.append(input_node)\n",
    "\n",
    "        if self.weight_method == 'attn':\n",
    "            normalized_weights = torch.softmax(self.edge_weights.to(dtype=dtype), dim=0)\n",
    "            out = torch.stack(nodes, dim=-1) * normalized_weights\n",
    "        elif self.weight_method == 'fastattn':\n",
    "            edge_weights = nn.functional.relu(self.edge_weights.to(dtype=dtype))\n",
    "            weights_sum = torch.sum(edge_weights)\n",
    "            out = torch.stack(\n",
    "                [(nodes[i] * edge_weights[i]) / (weights_sum + 0.0001) for i in range(len(nodes))], dim=-1)\n",
    "        elif self.weight_method == 'sum':\n",
    "            out = torch.stack(nodes, dim=-1)\n",
    "        else:\n",
    "            raise ValueError('unknown weight_method {}'.format(self.weight_method))\n",
    "        out = torch.sum(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Fnode(nn.Module):\n",
    "    \"\"\" A simple wrapper used in place of nn.Sequential for torchscript typing\n",
    "    Handles input type List[Tensor] -> output type Tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, combine: nn.Module, after_combine: nn.Module):\n",
    "        super(Fnode, self).__init__()\n",
    "        self.combine = combine\n",
    "        self.after_combine = after_combine\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n",
    "        return self.after_combine(self.combine(x))\n",
    "\n",
    "\n",
    "class BiFpnLayer(nn.Module):\n",
    "    def __init__(self, feature_info, feat_sizes, fpn_config, fpn_channels, num_levels=5, pad_type='',\n",
    "                 downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, act_layer=_ACT_LAYER,\n",
    "                 apply_resample_bn=False, pre_act=True, separable_conv=True, redundant_bias=False):\n",
    "        super(BiFpnLayer, self).__init__()\n",
    "        self.num_levels = num_levels\n",
    "        # fill feature info for all FPN nodes (chs and feat size) before creating FPN nodes\n",
    "        fpn_feature_info = feature_info + [\n",
    "            dict(num_chs=fpn_channels, size=feat_sizes[fc['feat_level']]) for fc in fpn_config.nodes]\n",
    "\n",
    "        self.fnode = nn.ModuleList()\n",
    "        for i, fnode_cfg in enumerate(fpn_config.nodes):\n",
    "            logging.debug('fnode {} : {}'.format(i, fnode_cfg))\n",
    "            combine = FpnCombine(\n",
    "                fpn_feature_info, fpn_channels, tuple(fnode_cfg['inputs_offsets']),\n",
    "                output_size=feat_sizes[fnode_cfg['feat_level']], pad_type=pad_type,\n",
    "                downsample=downsample, upsample=upsample, norm_layer=norm_layer, apply_resample_bn=apply_resample_bn,\n",
    "                redundant_bias=redundant_bias, weight_method=fnode_cfg['weight_method'])\n",
    "\n",
    "            after_combine = nn.Sequential()\n",
    "            conv_kwargs = dict(\n",
    "                in_channels=fpn_channels, out_channels=fpn_channels, kernel_size=3, padding=pad_type,\n",
    "                bias=False, norm_layer=norm_layer, act_layer=act_layer)\n",
    "            if pre_act:\n",
    "                conv_kwargs['bias'] = redundant_bias\n",
    "                conv_kwargs['act_layer'] = None\n",
    "                after_combine.add_module('act', act_layer(inplace=True))\n",
    "            after_combine.add_module(\n",
    "                'conv', SeparableConv2d(**conv_kwargs) if separable_conv else ConvBnAct2d(**conv_kwargs))\n",
    "\n",
    "            self.fnode.append(Fnode(combine=combine, after_combine=after_combine))\n",
    "\n",
    "        self.feature_info = fpn_feature_info[-num_levels::]\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        for fn in self.fnode:\n",
    "            x.append(fn(x))\n",
    "            \n",
    "#         for temp in x[-self.num_levels::]:\n",
    "#             print(temp.size())\n",
    "#         print(\"\")\n",
    "            \n",
    "        return x[-self.num_levels::]\n",
    "\n",
    "\n",
    "class BiFpn(nn.Module):\n",
    "\n",
    "    def __init__(self, config, feature_info):\n",
    "        super(BiFpn, self).__init__()\n",
    "        self.num_levels = config.num_levels\n",
    "        norm_layer = config.norm_layer or nn.BatchNorm2d\n",
    "        if config.norm_kwargs:\n",
    "            norm_layer = partial(norm_layer, **config.norm_kwargs)\n",
    "        act_layer = get_act_layer(config.act_type) or _ACT_LAYER\n",
    "        fpn_config = config.fpn_config or get_fpn_config(\n",
    "            config.fpn_name, min_level=config.min_level, max_level=config.max_level)\n",
    "\n",
    "        feat_sizes = get_feat_sizes(config.image_size, max_level=config.max_level)\n",
    "        prev_feat_size = feat_sizes[config.min_level]\n",
    "        self.resample = nn.ModuleDict()\n",
    "        for level in range(config.num_levels):\n",
    "            feat_size = feat_sizes[level + config.min_level]\n",
    "            if level < len(feature_info):\n",
    "                in_chs = feature_info[level]['num_chs']\n",
    "                feature_info[level]['size'] = feat_size\n",
    "            else:\n",
    "                # Adds a coarser level by downsampling the last feature map\n",
    "                self.resample[str(level)] = ResampleFeatureMap(\n",
    "                    in_channels=in_chs,\n",
    "                    out_channels=config.fpn_channels,\n",
    "                    input_size=prev_feat_size,\n",
    "                    output_size=feat_size,\n",
    "                    pad_type=config.pad_type,\n",
    "                    downsample=config.downsample_type,\n",
    "                    upsample=config.upsample_type,\n",
    "                    norm_layer=norm_layer,\n",
    "                    apply_bn=config.apply_resample_bn,\n",
    "                    redundant_bias=config.redundant_bias,\n",
    "                )\n",
    "                in_chs = config.fpn_channels\n",
    "                feature_info.append(dict(num_chs=in_chs, size=feat_size))\n",
    "            prev_feat_size = feat_size\n",
    "            \n",
    "\n",
    "        self.cell = SequentialList()\n",
    "        for rep in range(config.fpn_cell_repeats):\n",
    "            logging.debug('building cell {}'.format(rep))\n",
    "            fpn_layer = BiFpnLayer(\n",
    "                feature_info=feature_info,\n",
    "                feat_sizes=feat_sizes,\n",
    "                fpn_config=fpn_config,\n",
    "                fpn_channels=config.fpn_channels,\n",
    "                num_levels=config.num_levels,\n",
    "                pad_type=config.pad_type,\n",
    "                downsample=config.downsample_type,\n",
    "                upsample=config.upsample_type,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                separable_conv=config.separable_conv,\n",
    "                apply_resample_bn=config.apply_resample_bn,\n",
    "                pre_act=not config.conv_bn_relu_pattern,\n",
    "                redundant_bias=config.redundant_bias,\n",
    "            )\n",
    "            self.cell.add_module(str(rep), fpn_layer)\n",
    "            feature_info = fpn_layer.feature_info\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        for resample in self.resample.values():\n",
    "            x.append(resample(x[-1]))\n",
    "        x = self.cell(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class NAS_BiFpn(nn.Module):\n",
    "\n",
    "    def __init__(self, config, feature_info):\n",
    "        super(NAS_BiFpn, self).__init__()\n",
    "        self.num_levels = config.num_levels\n",
    "        norm_layer = config.norm_layer or nn.BatchNorm2d\n",
    "        if config.norm_kwargs:\n",
    "            norm_layer = partial(norm_layer, **config.norm_kwargs)\n",
    "        act_layer = get_act_layer(config.act_type) or _ACT_LAYER\n",
    "        fpn_config = config.fpn_config or get_fpn_config(\n",
    "            config.fpn_name, min_level=config.min_level, max_level=config.max_level)\n",
    "\n",
    "        feat_sizes = get_feat_sizes(config.image_size, max_level=config.max_level)\n",
    "        prev_feat_size = feat_sizes[config.min_level]\n",
    "        self.resample = nn.ModuleDict()\n",
    "        for level in range(config.num_levels):\n",
    "            feat_size = feat_sizes[level + config.min_level]\n",
    "            if level < len(feature_info):\n",
    "                in_chs = feature_info[level]['num_chs']\n",
    "                feature_info[level]['size'] = feat_size\n",
    "            else:\n",
    "                # Adds a coarser level by downsampling the last feature map\n",
    "                self.resample[str(level)] = ResampleFeatureMap(\n",
    "                    in_channels=in_chs,\n",
    "                    out_channels=config.fpn_channels,\n",
    "                    input_size=prev_feat_size,\n",
    "                    output_size=feat_size,\n",
    "                    pad_type=config.pad_type,\n",
    "                    downsample=config.downsample_type,\n",
    "                    upsample=config.upsample_type,\n",
    "                    norm_layer=norm_layer,\n",
    "                    apply_bn=config.apply_resample_bn,\n",
    "                    redundant_bias=config.redundant_bias,\n",
    "                )\n",
    "                in_chs = config.fpn_channels\n",
    "                feature_info.append(dict(num_chs=in_chs, size=feat_size))\n",
    "            prev_feat_size = feat_size\n",
    "            \n",
    "\n",
    "\n",
    "        self.cell = SequentialList()\n",
    "        for rep in range(config.fpn_cell_repeats):\n",
    "            logging.debug('building cell {}'.format(rep))\n",
    "            fpn_layer = BiFpnLayer(\n",
    "                feature_info=feature_info,\n",
    "                feat_sizes=feat_sizes,\n",
    "                fpn_config=fpn_config,\n",
    "                fpn_channels=config.fpn_channels,\n",
    "                num_levels=config.num_levels,\n",
    "                pad_type=config.pad_type,\n",
    "                downsample=config.downsample_type,\n",
    "                upsample=config.upsample_type,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                separable_conv=config.separable_conv,\n",
    "                apply_resample_bn=config.apply_resample_bn,\n",
    "                pre_act=not config.conv_bn_relu_pattern,\n",
    "                redundant_bias=config.redundant_bias,\n",
    "            )\n",
    "            self.cell.add_module(str(rep), fpn_layer)\n",
    "            feature_info = fpn_layer.feature_info\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        \n",
    "\n",
    "        \n",
    "        for resample in self.resample.values():\n",
    "            x.append(resample(x[-1]))\n",
    "\n",
    "        x_list = []\n",
    "\n",
    "        x_list.append(x)\n",
    "        \n",
    "        for layer in self.cell:\n",
    "            \n",
    "            x = layer([torch.clone(item) for item in x])\n",
    "            x_list.append(x)\n",
    "            \n",
    "        return x,x_list\n",
    "\n",
    "def bifpn_config(min_level, max_level, weight_method=None):\n",
    "    \"\"\"BiFPN config.\n",
    "    Adapted from https://github.com/google/automl/blob/56815c9986ffd4b508fe1d68508e268d129715c1/efficientdet/keras/fpn_configs.py\n",
    "    \"\"\"\n",
    "    p = OmegaConf.create()\n",
    "    weight_method = weight_method or 'fastattn'\n",
    "\n",
    "    num_levels = max_level - min_level + 1\n",
    "    node_ids = {min_level + i: [i] for i in range(num_levels)}\n",
    "\n",
    "    level_last_id = lambda level: node_ids[level][-1]\n",
    "    level_all_ids = lambda level: node_ids[level]\n",
    "    id_cnt = itertools.count(num_levels)\n",
    "\n",
    "    p.nodes = []\n",
    "    for i in range(max_level - 1, min_level - 1, -1):\n",
    "        # top-down path.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': [level_last_id(i), level_last_id(i + 1)],\n",
    "            'weight_method': weight_method,\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "\n",
    "    for i in range(min_level + 1, max_level + 1):\n",
    "        # bottom-up path.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': level_all_ids(i) + [level_last_id(i - 1)],\n",
    "            'weight_method': weight_method,\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "    return p\n",
    "\n",
    "\n",
    "def panfpn_config(min_level, max_level, weight_method=None):\n",
    "    \"\"\"PAN FPN config.\n",
    "    This defines FPN layout from Path Aggregation Networks as an alternate to\n",
    "    BiFPN, it does not implement the full PAN spec.\n",
    "    Paper: https://arxiv.org/abs/1803.01534\n",
    "    \"\"\"\n",
    "    p = OmegaConf.create()\n",
    "    weight_method = weight_method or 'fastattn'\n",
    "\n",
    "    num_levels = max_level - min_level + 1\n",
    "    node_ids = {min_level + i: [i] for i in range(num_levels)}\n",
    "    level_last_id = lambda level: node_ids[level][-1]\n",
    "    id_cnt = itertools.count(num_levels)\n",
    "\n",
    "    p.nodes = []\n",
    "    for i in range(max_level, min_level - 1, -1):\n",
    "        # top-down path.\n",
    "        offsets = [level_last_id(i), level_last_id(i + 1)] if i != max_level else [level_last_id(i)]\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': offsets,\n",
    "            'weight_method': weight_method,\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "\n",
    "    for i in range(min_level, max_level + 1):\n",
    "        # bottom-up path.\n",
    "        offsets = [level_last_id(i), level_last_id(i - 1)] if i != min_level else [level_last_id(i)]\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': offsets,\n",
    "            'weight_method': weight_method,\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def qufpn_config(min_level, max_level, weight_method=None):\n",
    "    \"\"\"A dynamic quad fpn config that can adapt to different min/max levels.\n",
    "    It extends the idea of BiFPN, and has four paths:\n",
    "        (up_down -> bottom_up) + (bottom_up -> up_down).\n",
    "    Paper: https://ieeexplore.ieee.org/document/9225379\n",
    "    Ref code: From contribution to TF EfficientDet\n",
    "    https://github.com/google/automl/blob/eb74c6739382e9444817d2ad97c4582dbe9a9020/efficientdet/keras/fpn_configs.py\n",
    "    \"\"\"\n",
    "    p = OmegaConf.create()\n",
    "    weight_method = weight_method or 'fastattn'\n",
    "    quad_method = 'fastattn'\n",
    "    num_levels = max_level - min_level + 1\n",
    "    node_ids = {min_level + i: [i] for i in range(num_levels)}\n",
    "    level_last_id = lambda level: node_ids[level][-1]\n",
    "    level_all_ids = lambda level: node_ids[level]\n",
    "    level_first_id = lambda level: node_ids[level][0]\n",
    "    id_cnt = itertools.count(num_levels)\n",
    "\n",
    "    p.nodes = []\n",
    "    for i in range(max_level - 1, min_level - 1, -1):\n",
    "        # top-down path 1.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': [level_last_id(i), level_last_id(i + 1)],\n",
    "            'weight_method': weight_method\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "    node_ids[max_level].append(node_ids[max_level][-1])\n",
    "\n",
    "    for i in range(min_level + 1, max_level):\n",
    "        # bottom-up path 2.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': level_all_ids(i) + [level_last_id(i - 1)],\n",
    "            'weight_method': weight_method\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "\n",
    "    i = max_level\n",
    "    p.nodes.append({\n",
    "        'feat_level': i,\n",
    "        'inputs_offsets': [level_first_id(i)] + [level_last_id(i - 1)],\n",
    "        'weight_method': weight_method\n",
    "    })\n",
    "    node_ids[i].append(next(id_cnt))\n",
    "    node_ids[min_level].append(node_ids[min_level][-1])\n",
    "\n",
    "    for i in range(min_level + 1, max_level + 1, 1):\n",
    "        # bottom-up path 3.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': [\n",
    "                level_first_id(i), level_last_id(i - 1) if i != min_level + 1 else level_first_id(i - 1)],\n",
    "            'weight_method': weight_method\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "    node_ids[min_level].append(node_ids[min_level][-1])\n",
    "\n",
    "    for i in range(max_level - 1, min_level, -1):\n",
    "        # top-down path 4.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': [node_ids[i][0]] + [node_ids[i][-1]] + [level_last_id(i + 1)],\n",
    "            'weight_method': weight_method\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "    i = min_level\n",
    "    p.nodes.append({\n",
    "        'feat_level': i,\n",
    "        'inputs_offsets': [node_ids[i][0]] + [level_last_id(i + 1)],\n",
    "        'weight_method': weight_method\n",
    "    })\n",
    "    node_ids[i].append(next(id_cnt))\n",
    "    node_ids[max_level].append(node_ids[max_level][-1])\n",
    "\n",
    "    # NOTE: the order of the quad path is reversed from the original, my code expects the output of\n",
    "    # each FPN repeat to be same as input from backbone, in order of increasing reductions\n",
    "    for i in range(min_level, max_level + 1):\n",
    "        # quad-add path.\n",
    "        p.nodes.append({\n",
    "            'feat_level': i,\n",
    "            'inputs_offsets': [node_ids[i][2], node_ids[i][4]],\n",
    "            'weight_method': quad_method\n",
    "        })\n",
    "        node_ids[i].append(next(id_cnt))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_fpn_config(fpn_name, min_level=3, max_level=7):\n",
    "    if not fpn_name:\n",
    "        fpn_name = 'bifpn_fa'\n",
    "    name_to_config = {\n",
    "        'bifpn_sum': bifpn_config(min_level=min_level, max_level=max_level, weight_method='sum'),\n",
    "        'bifpn_attn': bifpn_config(min_level=min_level, max_level=max_level, weight_method='attn'),\n",
    "        'bifpn_fa': bifpn_config(min_level=min_level, max_level=max_level, weight_method='fastattn'),\n",
    "        'pan_sum': panfpn_config(min_level=min_level, max_level=max_level, weight_method='sum'),\n",
    "        'pan_fa': panfpn_config(min_level=min_level, max_level=max_level, weight_method='fastattn'),\n",
    "        'qufpn_sum': qufpn_config(min_level=min_level, max_level=max_level, weight_method='sum'),\n",
    "        'qufpn_fa': qufpn_config(min_level=min_level, max_level=max_level, weight_method='fastattn'),\n",
    "    }\n",
    "    return name_to_config[fpn_name]\n",
    "\n",
    "\n",
    "def get_feat_sizes(image_size: Tuple[int, int], max_level: int):\n",
    "    \"\"\"Get feat widths and heights for all levels.\n",
    "    Args:\n",
    "      image_size: a tuple (H, W)\n",
    "      max_level: maximum feature level.\n",
    "    Returns:\n",
    "      feat_sizes: a list of tuples (height, width) for each level.\n",
    "    \"\"\"\n",
    "    feat_size = image_size\n",
    "    feat_sizes = [feat_size]\n",
    "    for _ in range(1, max_level + 1):\n",
    "        feat_size = ((feat_size[0] - 1) // 2 + 1, (feat_size[1] - 1) // 2 + 1)\n",
    "        feat_sizes.append(feat_size)\n",
    "    return feat_sizes\n",
    "\n",
    "def get_feature_info(backbone):\n",
    "    if isinstance(backbone.feature_info, Callable):\n",
    "        # old accessor for timm versions <= 0.1.30, efficientnet and mobilenetv3 and related nets only\n",
    "        feature_info = [dict(num_chs=f['num_chs'], reduction=f['reduction'])\n",
    "                        for i, f in enumerate(backbone.feature_info())]\n",
    "    else:\n",
    "        # new feature info accessor, timm >= 0.2, all models supported\n",
    "        feature_info = backbone.feature_info.get_dicts(keys=['num_chs', 'reduction'])\n",
    "    return feature_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be the same shape\n",
    "class ReshapeInputLayer(nn.Module):\n",
    "    def __init__(self, L, args):\n",
    "        super(ReshapeInputLayer, self).__init__()\n",
    "        self.C = args.C\n",
    "        self.L = L\n",
    "        self.conv = nn.Conv1d(self.C,self.C,L,L)\n",
    "        self.bn = nn.BatchNorm1d(self.C)\n",
    "        self.dropout = nn.Dropout(args.drpt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),x.size(1),self.L**2)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be the same shape\n",
    "class ReshapeOutputLayer(nn.Module):\n",
    "    def __init__(self, L, args):\n",
    "        super(ReshapeOutputLayer, self).__init__()\n",
    "        self.C = args.C\n",
    "        self.L = L\n",
    "        self.tconv = nn.ConvTranspose1d(self.C,self.C,L,L)\n",
    "        self.bn = nn.BatchNorm1d(self.C)\n",
    "        self.dropout = nn.Dropout(args.drpt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tconv(x)\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(out.size(0),out.size(1),self.L,self.L)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_Fusion_Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_classes,args):\n",
    "        super(Att_Fusion_Net, self).__init__()\n",
    "\n",
    "        self.config = effdet.config.model_config.get_efficientdet_config('efficientdetv2_dt')\n",
    "        self.config.num_classes = num_classes\n",
    "\n",
    "        thermal_det = EfficientDet(self.config)\n",
    "        rgb_det = EfficientDet(self.config)\n",
    "\n",
    "        self.thermal_backbone = thermal_det.backbone\n",
    "        thermal_feature_info = get_feature_info(self.thermal_backbone)\n",
    "        self.thermal_fpn = NAS_BiFpn(self.config,thermal_feature_info)\n",
    "\n",
    "\n",
    "        self.rgb_backbone = rgb_det.backbone\n",
    "        rgb_feature_info = get_feature_info(self.rgb_backbone)\n",
    "        self.rgb_fpn = NAS_BiFpn(self.config,rgb_feature_info)\n",
    "        \n",
    "        self.rgb_reshape_layers = self.create_input_reshape_layers(args)\n",
    "        self.thermal_reshape_layers = self.create_input_reshape_layers(args)\n",
    "        \n",
    "    def create_input_reshape_layers(self, args):\n",
    "        L = [96, 48, 24, 12, 6]\n",
    "        input_reshape_layers = nn.ModuleList()\n",
    "        for i in range(args.fusion_levels):\n",
    "            temp = nn.ModuleList()\n",
    "            for j in range(len(L)):\n",
    "                temp.append(ReshapeInputLayer(L[j],args))\n",
    "#             input_reshape_layers.append([ReshapeInputLayer(L[j],args) for j in range(len(L))])\n",
    "            input_reshape_layers.append(temp)\n",
    "    \n",
    "        return input_reshape_layers\n",
    "\n",
    "\n",
    "    def forward(self, data_pair, branch='fusion'):\n",
    "        thermal_x, rgb_x = data_pair[0], data_pair[1]\n",
    "        \n",
    "        thermal_x, rgb_x = self.thermal_backbone(thermal_x), self.rgb_backbone(rgb_x)\n",
    "        \n",
    "        _, thermal_list = self.thermal_fpn(thermal_x)\n",
    "        \n",
    "        _, rgb_list = self.rgb_fpn(rgb_x)\n",
    "        \n",
    "        new_thermal_list = []\n",
    "        new_rgb_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(args.fusion_levels):\n",
    "            new_thermal_list.append([])\n",
    "            new_rgb_list.append([])\n",
    "            \n",
    "            new_thermal_list[i] += [self.thermal_reshape_layers[j][i](thermal_list[1:][j][i]) for j in range(len(thermal_list[1:]))]\n",
    "            new_rgb_list[i] += [self.rgb_reshape_layers[j][i](rgb_list[1:][j][i]) for j in range(len(rgb_list[1:]))]\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        return new_thermal_list, new_rgb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map(dict):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Map, self).__init__(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, dict):\n",
    "                for k, v in arg.items():\n",
    "                    self[k] = v\n",
    "\n",
    "        if kwargs:\n",
    "            for k, v in kwargs.items():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__setitem__(key, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super(Map, self).__setitem__(key, value)\n",
    "        self.__dict__.update({key: value})\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__delitem__(item)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super(Map, self).__delitem__(key)\n",
    "        del self.__dict__[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 96])\n",
      "torch.Size([1, 128, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "args = Map({\"C\":128,\"drpt\":0.5})\n",
    "m1 = ReshapeInputLayer(96,args)\n",
    "m2 = ReshapeOutputLayer(96,args)\n",
    "\n",
    "out1 = m1(torch.rand(1,128,96,96))\n",
    "print(out1.shape)\n",
    "out2 = m2(out1)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Map({\"C\":128,\"drpt\":0.5,\"fusion_levels\":5})\n",
    "model = Att_Fusion_Net(90,args)\n",
    "a,b = model([torch.rand(1,3,768,768),torch.rand(1,3,768,768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 96])\n",
      "torch.Size([1, 128, 96])\n",
      "torch.Size([1, 128, 96])\n",
      "torch.Size([1, 128, 96])\n",
      "torch.Size([1, 128, 96])\n",
      "\n",
      "torch.Size([1, 128, 48])\n",
      "torch.Size([1, 128, 48])\n",
      "torch.Size([1, 128, 48])\n",
      "torch.Size([1, 128, 48])\n",
      "torch.Size([1, 128, 48])\n",
      "\n",
      "torch.Size([1, 128, 24])\n",
      "torch.Size([1, 128, 24])\n",
      "torch.Size([1, 128, 24])\n",
      "torch.Size([1, 128, 24])\n",
      "torch.Size([1, 128, 24])\n",
      "\n",
      "torch.Size([1, 128, 12])\n",
      "torch.Size([1, 128, 12])\n",
      "torch.Size([1, 128, 12])\n",
      "torch.Size([1, 128, 12])\n",
      "torch.Size([1, 128, 12])\n",
      "\n",
      "torch.Size([1, 128, 6])\n",
      "torch.Size([1, 128, 6])\n",
      "torch.Size([1, 128, 6])\n",
      "torch.Size([1, 128, 6])\n",
      "torch.Size([1, 128, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a1 in a:\n",
    "    for a2 in a1:\n",
    "        print(a2.size())\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 96])\n"
     ]
    }
   ],
   "source": [
    "for a2 in a[0]:\n",
    "        print(a2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
